{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same as before but this time with an Agent!\n",
    "\n",
    "\n",
    "We start with the same import statements like before. We also setup the LunarLander environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from gymclass import Notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "env = env.unwrapped\n",
    "env.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we setup the Agent \n",
    "\n",
    "![Agent](images/Agent.png)\n",
    "\n",
    "This agent is a special kind of Reinforced Machine Learning agent called a **Policy Gradient**.\n",
    "\n",
    "The objective of this agent is to maximize the “expected” reward. When controlling something complicated with many steps like a lunar lunder the policy gradent agent must figure out what sequence of actions will lead to the highest rewards. \n",
    "\n",
    "Like atheletes must practice to build muscle memory our Policy Gradient must practice and train to learn the best actions to take. \n",
    "\n",
    "## How the code works \n",
    "\n",
    "![Agent](images/LearningExplain.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use your own AI in the next testing steps change **MYNAME** in the save string below to be your name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from collections import deque\n",
    "from dqn_agent import DQNAgent\n",
    "import time\n",
    "\n",
    "# eps stands for Epsilon which is the amount of the Agent will randomly take actions \n",
    "# its starts being quite random and this causes it to 'explore' its options and potentially\n",
    "# try things that might not work as well.  As time passes in the training we allow less \n",
    "# random actions. \n",
    "\n",
    "eps_start=1.0\n",
    "eps_end=0.001\n",
    "eps_decay=0.995\n",
    "eps = eps_start  # initialize epsilon\n",
    "\n",
    "episode_rewards = [] # List of all rewards\n",
    "episode_rewards_window = deque(maxlen=100)  # last 100 scores\n",
    "\n",
    "\n",
    "# Replace MYNAME with your name below. \n",
    "save_path = \"output/LunarLander-ROBIN.ckpt\"\n",
    "agent = DQNAgent(state_size=8, action_size=4, seed=0, hidden_layer1=64, hidden_layer2=108)\n",
    "\n",
    "episodes = 2000\n",
    "for t in Notebook.log_progress(range(episodes)):\n",
    "    observation = env.reset()\n",
    "    episode_reward = 0\n",
    "    tic = time.perf_counter() \n",
    "    while True:\n",
    "        # 1. Choose an action based on observation        \n",
    "        action = agent.act(observation, eps)\n",
    "        \n",
    "        # 2. Take action in the environment\n",
    "        observation_next, reward, done, info = env.step(action)\n",
    "        \n",
    "        # 3. Now tell the agent about the action and reward so it can learn\n",
    "        agent.step(observation, action, reward, observation_next, done)\n",
    "        \n",
    "        # Taking too long\n",
    "        #if time.perf_counter() - tic > 60:\n",
    "        #    done = True\n",
    "\n",
    "        # Oops Crashed or flew away, stops early \n",
    "        if episode_reward<-500:\n",
    "            done = True\n",
    "\n",
    "        # After initial training quit early when things go wrong \n",
    "        # try to amplify good experience, remove random \n",
    "        if t>500 and episode_reward<-250:\n",
    "            done = True\n",
    "\n",
    "            \n",
    "        observation = observation_next\n",
    "        episode_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    # save scores and update epsilon which sets the amount of random exploration\n",
    "    episode_rewards_window.append(episode_reward)\n",
    "    episode_rewards.append(episode_reward)\n",
    "    eps = max(eps_end, eps_decay*eps)\n",
    "    raw = np.mean(episode_rewards_window)\n",
    "    print(\"\\r Episodes \", t, \" Current Rolling Avg Reward \", raw, end=\"\")   \n",
    "    if raw > 350:\n",
    "        break;\n",
    "    \n",
    "        \n",
    "        \n",
    "agent.save(save_path)  \n",
    "agent.save_bin(save_path+'.bin')  \n",
    "print(\"\")\n",
    "print(\"Done! Average Reward =\", np.mean(episode_rewards_window))\n",
    "print(\"Average Fitness Score =\", agent.fitness(np.mean(episode_rewards_window)))\n",
    "plt.plot(np.arange(len(episode_rewards)), episode_rewards)\n",
    "plt.ylabel('Reward')\n",
    "plt.xlabel('Training Steps')\n",
    "plt.savefig(\"reward-episodes-\" +str(episodes)+\".png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
